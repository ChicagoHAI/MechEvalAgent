# GLOBAL EVALUATION MODE (You are not allowed to modify the plan, code and documentation files.)

**Evaluation Mode: No-Execution Code Evaluation**

You are **not allowed** to:

* Execute code or rerun experiments
* Write or run new code to verify behavior
* Simulate outputs or construct test cases

You are **allowed** to:

* Read the files you are permitted to read under the rules below
* Judge correctness and runnability by **static inspection only**

All judgments must be based **solely on explicit evidence in the allowed files**.
If a property cannot be verified by static inspection, it **must not be assumed**.

Here is the repo_path:
`{REPO_PATH}`

---

### **1. Code Evaluation** (You are not allowed to modify the plan, code and documentation files.))

You are a **strict, deterministic evaluator** of the code implementing the circuit analysis.

You will read **all code under the REPO**:
`{REPO_PATH}`

### **Source of Project Goal**

Use the following files:

1. The **Plan** file.
2. The **codewalk** file (always used).

These sources define the intended method and expected outputs.

Do **not** read the documentation file unless explicitly instructed.

Use the **codewalk file** as your main guide to which scripts / notebooks implement the core analysis and how they are supposed to work.

If the implementation is in Jupyter notebooks, treat each **code cell** as a unit.
If the implementation is in plain Python scripts or modules, treat each **top-level function** (or clearly separated code block) as a unit.

---

### **1.1 Per-block Evaluation**

For **every code block / function** that participates in the main analysis:

You must **evaluate it by static inspection only** (no execution).
For each block, record the following **binary flags** and a short factual note whenever a flag is `N`:

1. **Runnable (Y/N)**

   * **Y**: The block is **syntactically valid**, has required imports/definitions available in scope as written, and has no obvious runtime blockers (undefined variables, missing imports, impossible paths, interactive prompts, or unconditional infinite loops).
   * **N**: The block has a clear static issue that would prevent execution (syntax errors, undefined variables, missing imports, unresolved symbols, missing files referenced as constants, obvious infinite loop/hang patterns, or required manual intervention).

   Do **not** assume hidden environment fixes.

2. **Correct-Implementation (Y/N)**

   * **Y**: The logic appears to implement the described computation correctly by static inspection (indexing, metric formulas, patching logic, dataset handling), consistent with the Plan/codewalk intent.
   * **N**: There is a clear implementation error relative to the stated purpose, **or** correctness depends on runtime behavior that cannot be verified and is not justified by the code structure.

3. **Redundant (Y/N)**

   * **Y**: The block duplicates another block’s computation without adding new information. (Revising previous wrong results does not considered as redundant.)
   * **N**: Non-duplicative.

4. **Irrelevant (Y/N)**

   * **Y**: The block does not contribute to achieving the project goal as defined in the Plan/codewalk.
   * **N**: Relevant.

Record these flags in a **table** inside the notebook (one row per block/function, one column per flag), with a clear identifier for each block (file name + cell index or function name).
If any flag is `N`, include a **brief factual error note** (1–2 sentences, no subjective language).

---

### **1.2 Quantitative Metrics**

From the per-block table, compute the following **objective percentages**:

* **Runnable%** = (# blocks with `Runnable = Y`) / (total blocks) × 100
* **Output-Matches-Expectation%** = (# blocks with `Output-Matches-Expectation = Y`) / (total blocks) × 100
* **Incorrect%** = (# blocks with `Correct-Implementation = N`) / (total blocks) × 100
* **Redundant%** = (# blocks with `Redundant = Y`) / (total blocks) × 100
* **Irrelevant%** = (# blocks with `Irrelevant = Y`) / (total blocks) × 100

If, during evaluation, you identify a clear static fix that would resolve a failing block, you may describe the fix in notes **without modifying code**. In that case, also compute:

* **Correction-Rate%** = (# blocks with identified corrections) / (# blocks that ever failed `Runnable` or `Correct-Implementation`) × 100

All percentages must be explicit numeric values (0–100), directly computed from the binary table.

---

### **1.3 Binary Checklist Summary**

At the **end of the evaluation**, produce a **binary checklist** summarizing whether *any violations exist*.

1. **C1: All core analysis code is runnable. (PASS/FAIL)**

   * **PASS** if no block has `Runnable = N`.
   * **FAIL** otherwise.

2. **C2: All implementations are correct. (PASS/FAIL)**

   * **PASS** if no block has `Correct-Implementation = N`.
   * **FAIL** otherwise.

3. **C3: No redundant code. (PASS/FAIL)**

   * **PASS** if no block has `Redundant = Y`.
   * **FAIL** otherwise.

4. **C4: No irrelevant code. (PASS/FAIL)**

   * **PASS** if no block has `Irrelevant = Y`.
   * **FAIL** otherwise.

Output this checklist in a **clear table** with columns:
`Checklist Item | Condition | PASS/FAIL`

---

### **1.4 Output Format and Location**

You must produce **two files**:

---

#### **A. Jupyter Notebook**

**Path:** `{REPO_PATH}/no_exe_evaluation/code_critic_evaluation.ipynb`

The notebook must include, in order:

1. **Block-level table** with all binary flags **and error notes**.
2. **Quantitative metrics** (Runnable%, Output-Matches-Expectation%, Incorrect%, Redundant%, Irrelevant%, Correction-Rate%).
3. **Binary checklist summary** (C1–C4).

Add a summary of all those at the end of the file.

No other files may be produced except the JSON summary below.

---

#### **B. JSON Summary File**

**Path:** `{REPO_PATH}/no_exe_evaluation/code_critic_summary.json`

It must contain the following structure:

```json
{{
  "Runnable_Percentage": float,
  "Incorrect_Percentage": float,
  "Redundant_Percentage": float,
  "Irrelevant_Percentage": float,
  "Correction_Rate_Percentage": float,

  "Issues": {{
    "Runnable_Issues_Exist": boolean,
    "Output_Mismatch_Exists": boolean,
    "Incorrect_Exists": boolean,
    "Redundant_Exists": boolean,
    "Irrelevant_Exists": boolean
  }},

  "Checklist": {{
    "C1_All_Runnable": "PASS" or "FAIL",
    "C2_All_Correct": "PASS" or "FAIL",
    "C3_No_Redundant": "PASS" or "FAIL",
    "C4_No_Irrelevant": "PASS" or "FAIL"
  }},

  "Rationale":{{
      "C1_All_Runnable": Why "PASS" or "FAIL",
      "C2_All_Correct": Why "PASS" or "FAIL",
      "C3_No_Redundant": Why "PASS" or "FAIL",
      "C4_No_Irrelevant": Why "PASS" or "FAIL"
  }}
}}
```